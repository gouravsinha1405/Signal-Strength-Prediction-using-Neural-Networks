{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import statsmodels.api as sm \n",
    "from scipy.stats import gamma\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import pickle \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "#-------Importing tensorflow libraries-------#\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense,Dropout\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "#------split and pipeline libraries------#\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_signal(row):\n",
    "    \"\"\"Feature Engineering to categorise signal strength for classifier model\"\"\"\n",
    "    if row >= 7:\n",
    "        return 3\n",
    "    elif row <=4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "        \n",
    "def classifier_model_architecture(input_dim,output_dim):\n",
    "    \"\"\"Defines the generic architecture of the classifier neural network\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def principal_components(data):\n",
    "    \"\"\"Convert the input features into principal components\"\"\"\n",
    "    scaled = data.copy()\n",
    "    #----Column standardization---------#\n",
    "    for column in scaled.drop(columns=['Signal_Strength']).columns:\n",
    "        scaled[column] = scaled[column].apply(lambda x:\n",
    "                        (x - scaled[column].mean())/scaled[column].std()\n",
    "                       )\n",
    "    #-----Decomposition into principal components at n_components = 6 which we found during EDA-----#\n",
    "    features = list(scaled.drop(columns=['Signal_Strength']).columns)\n",
    "    pca = PCA(n_components=6)\n",
    "    principalComponents = pca.fit_transform(np.array(scaled[features]))\n",
    "    pcDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1', 'PC2','PC3','PC4','PC5','PC6'])\n",
    "    finalDf = pd.concat([pcDf,scaled[['Signal_Strength']]], axis=1)\n",
    "    return finalDf\n",
    "\n",
    "def backwardElimination(x, Y, significance_level, columns):\n",
    "    \"\"\"Backward elimination method\"\"\"\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues)\n",
    "        if maxVar > significance_level:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j] == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "\n",
    "def import_csv_data():\n",
    "    \"\"\"Code to import csv data\"\"\"\n",
    "    global v,csv_check,df\n",
    "    csv_file_path = askopenfilename()\n",
    "    print(csv_file_path)\n",
    "    v.set(csv_file_path)\n",
    "    try:\n",
    "        csv_check.set(\"\")\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        csv_check.set(\"Done\")\n",
    "    except:\n",
    "        csv_check.set(\"Not a CSV\")\n",
    "    \n",
    "def check_target():\n",
    "    \"\"\"To check if dependent variable is present or not\"\"\"\n",
    "    global target,target_found\n",
    "    if target.get() in df.columns:\n",
    "        target_found.set(\"Found\")\n",
    "    else:\n",
    "        target_found.set(\"Not Found\")\n",
    "\n",
    "def regressor_train():\n",
    "    \"\"\"Train Regressor model\"\"\"\n",
    "    global model1,trained\n",
    "    df1 = df.copy()\n",
    "    #-----Imputing outliers------------#\n",
    "    for column in df1.drop(columns=['Signal_Strength']).columns:\n",
    "        df1[column] = df1[column].apply(lambda x: x if \n",
    "                        ((x - df1[column].mean())/df1[column].std()) <= 3 \n",
    "                        else\n",
    "                        df1[column].median()\n",
    "                        #np.nan\n",
    "                       )\n",
    "    #------------Applying backward elimination to filter out columns----------#\n",
    "    x_ols = df1.drop(columns=['Signal_Strength']).values\n",
    "    y_ols = df1['Signal_Strength']\n",
    "    significance_level = 0.05\n",
    "    selected_columns = df1.drop(columns=['Signal_Strength']).columns\n",
    "    data, selected_columns = backwardElimination(x_ols, y_ols, significance_level, selected_columns)\n",
    "    #---------Retaining only selected columns obtained after backward elimination---------------------------#\n",
    "    selected_columns = list(selected_columns)\n",
    "    selected_columns.append('Signal_Strength')\n",
    "    After_backward_elimination = df[selected_columns].copy()\n",
    "    #---------------Column Standardization----------------------#\n",
    "    for column in After_backward_elimination.drop(columns=['Signal_Strength']).columns:\n",
    "        After_backward_elimination[column] = After_backward_elimination[column].apply(lambda x:\n",
    "                        (x - After_backward_elimination[column].mean())/After_backward_elimination[column].std()\n",
    "                       )\n",
    "    #---------------------Data split--------------------------#\n",
    "    X = After_backward_elimination.drop(columns=['Signal_Strength']).values\n",
    "    Y = After_backward_elimination['Signal_Strength'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.33, random_state=42)\n",
    "    #--------Defining the Model architecture------------------------#\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(10, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model1.add(Dropout(0.25))\n",
    "    model1.add(Dense(10, activation='relu'))\n",
    "    model1.add(Dropout(0.25))\n",
    "    model1.add(Dense(1, activation='linear'))\n",
    "    #-------------Training the model-------------#\n",
    "    try:\n",
    "        trained.set(\"\")\n",
    "        model1.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "        model1.fit(X_train, y_train, epochs=200, batch_size=10,  verbose=1, validation_split=0.2)\n",
    "        trained.set(\"Network trained\")\n",
    "    except:\n",
    "        trained.set(\"Error while Training the network\")\n",
    "\n",
    "def save_regressor_model():\n",
    "    \"\"\"Save Regressor model\"\"\"\n",
    "    global saved\n",
    "    try:\n",
    "        saved.set(\"\")\n",
    "        model1.save(\"Signal_Regressor_model.h5\")\n",
    "        saved.set(\"Saved to the Disk\")\n",
    "    except:\n",
    "        saved.set(\"Not able to save due to the error\")\n",
    "\n",
    "def classifier_train():\n",
    "    \"\"\"Train classifier model\"\"\"\n",
    "    global model2, c_trained\n",
    "    df2 = df.copy()\n",
    "    #------Imputing outliers with median values-----#\n",
    "    for column in df2.drop(columns=['Signal_Strength']).columns:\n",
    "        df2[column] = df2[column].apply(lambda x: x if\n",
    "                        ((x - df2[column].mean())/df2[column].std()) <= 3 \n",
    "                        else\n",
    "                        df2[column].median()\n",
    "                       )\n",
    "    #---------Feature engineering to categorise signal strength into 1,2 and 3-----------#\n",
    "    df2['Signal_Strength'] = df2['Signal_Strength'].apply(categorise_signal)\n",
    "    #----------getting principal components--------#\n",
    "    finalDf = principal_components(df2)\n",
    "    #------categorising the target variables and splitting the data------#\n",
    "    categorical_labels = to_categorical(finalDf['Signal_Strength'], num_classes=None)\n",
    "    xpca = finalDf.drop(columns=['Signal_Strength']).values\n",
    "    ypca = categorical_labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(xpca, ypca,test_size=0.33, random_state=42)\n",
    "    #---------defining model architecture----------#\n",
    "    model2 = classifier_model_architecture(6,4)\n",
    "    #---------Train the model------------------#\n",
    "    try:\n",
    "        c_trained.set(\"\")\n",
    "        model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model2.fit(X_train, y_train, epochs=150, batch_size=10,  verbose=1, validation_split=0.2)\n",
    "        c_trained.set(\"Network trained\")\n",
    "    except:\n",
    "        c_trained.set(\"Error while Training the network\")\n",
    "\n",
    "def save_classifier_model():\n",
    "    \"\"\"Saving Classifier model\"\"\"\n",
    "    global c_saved\n",
    "    try:\n",
    "        c_saved.set(\"\")\n",
    "        model2.save(\"Signal_Classification_model.h5\")\n",
    "        c_saved.set(\"Saved to the Disk\")\n",
    "    except:\n",
    "        c_saved.set(\"Not able to save due to the error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tkinter GUI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/DELL/Downloads/GL_Assignment_NeuralNetwork/Part- 1,2&3 - Signal.csv\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 856 samples, validate on 215 samples\n",
      "Epoch 1/200\n",
      "856/856 [==============================] - 0s 391us/sample - loss: 28.0968 - mean_squared_error: 28.0967 - mean_absolute_error: 5.2218 - val_loss: 23.8504 - val_mean_squared_error: 23.8504 - val_mean_absolute_error: 4.7994\n",
      "Epoch 2/200\n",
      "856/856 [==============================] - 0s 119us/sample - loss: 16.8039 - mean_squared_error: 16.8039 - mean_absolute_error: 3.8827 - val_loss: 9.5178 - val_mean_squared_error: 9.5178 - val_mean_absolute_error: 2.8908\n",
      "Epoch 3/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 5.9206 - mean_squared_error: 5.9206 - mean_absolute_error: 2.0649 - val_loss: 2.1910 - val_mean_squared_error: 2.1910 - val_mean_absolute_error: 1.2148\n",
      "Epoch 4/200\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 3.7097 - mean_squared_error: 3.7097 - mean_absolute_error: 1.5194 - val_loss: 1.2467 - val_mean_squared_error: 1.2467 - val_mean_absolute_error: 0.9148\n",
      "Epoch 5/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 3.3885 - mean_squared_error: 3.3885 - mean_absolute_error: 1.4984 - val_loss: 0.8361 - val_mean_squared_error: 0.8361 - val_mean_absolute_error: 0.7296\n",
      "Epoch 6/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 3.3173 - mean_squared_error: 3.3173 - mean_absolute_error: 1.4591 - val_loss: 0.7068 - val_mean_squared_error: 0.7068 - val_mean_absolute_error: 0.6755\n",
      "Epoch 7/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 2.7509 - mean_squared_error: 2.7509 - mean_absolute_error: 1.3275 - val_loss: 0.6723 - val_mean_squared_error: 0.6723 - val_mean_absolute_error: 0.6580\n",
      "Epoch 8/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 2.6384 - mean_squared_error: 2.6384 - mean_absolute_error: 1.2994 - val_loss: 0.6197 - val_mean_squared_error: 0.6197 - val_mean_absolute_error: 0.6326\n",
      "Epoch 9/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 2.5904 - mean_squared_error: 2.5904 - mean_absolute_error: 1.3002 - val_loss: 0.5013 - val_mean_squared_error: 0.5013 - val_mean_absolute_error: 0.5488\n",
      "Epoch 10/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 2.3860 - mean_squared_error: 2.3860 - mean_absolute_error: 1.2382 - val_loss: 0.5692 - val_mean_squared_error: 0.5692 - val_mean_absolute_error: 0.5913\n",
      "Epoch 11/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 2.3508 - mean_squared_error: 2.3508 - mean_absolute_error: 1.2089 - val_loss: 0.5088 - val_mean_squared_error: 0.5088 - val_mean_absolute_error: 0.5538\n",
      "Epoch 12/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 2.0846 - mean_squared_error: 2.0846 - mean_absolute_error: 1.1531 - val_loss: 0.5112 - val_mean_squared_error: 0.5112 - val_mean_absolute_error: 0.5627\n",
      "Epoch 13/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 2.1788 - mean_squared_error: 2.1788 - mean_absolute_error: 1.1618 - val_loss: 0.4021 - val_mean_squared_error: 0.4021 - val_mean_absolute_error: 0.4784\n",
      "Epoch 14/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 1.9994 - mean_squared_error: 1.9994 - mean_absolute_error: 1.1147 - val_loss: 0.4206 - val_mean_squared_error: 0.4206 - val_mean_absolute_error: 0.4991\n",
      "Epoch 15/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 2.0785 - mean_squared_error: 2.0785 - mean_absolute_error: 1.1373 - val_loss: 0.4198 - val_mean_squared_error: 0.4198 - val_mean_absolute_error: 0.4977\n",
      "Epoch 16/200\n",
      "856/856 [==============================] - 0s 122us/sample - loss: 1.8786 - mean_squared_error: 1.8786 - mean_absolute_error: 1.1118 - val_loss: 0.5041 - val_mean_squared_error: 0.5041 - val_mean_absolute_error: 0.5627\n",
      "Epoch 17/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 1.9244 - mean_squared_error: 1.9244 - mean_absolute_error: 1.0952 - val_loss: 0.4935 - val_mean_squared_error: 0.4935 - val_mean_absolute_error: 0.5472\n",
      "Epoch 18/200\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 1.8671 - mean_squared_error: 1.8671 - mean_absolute_error: 1.0799 - val_loss: 0.4210 - val_mean_squared_error: 0.4210 - val_mean_absolute_error: 0.5052\n",
      "Epoch 19/200\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 1.9913 - mean_squared_error: 1.9913 - mean_absolute_error: 1.1197 - val_loss: 0.4324 - val_mean_squared_error: 0.4324 - val_mean_absolute_error: 0.5088\n",
      "Epoch 20/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 1.7162 - mean_squared_error: 1.7162 - mean_absolute_error: 1.0587 - val_loss: 0.3979 - val_mean_squared_error: 0.3979 - val_mean_absolute_error: 0.4811\n",
      "Epoch 21/200\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 1.8138 - mean_squared_error: 1.8138 - mean_absolute_error: 1.0632 - val_loss: 0.4379 - val_mean_squared_error: 0.4379 - val_mean_absolute_error: 0.5180\n",
      "Epoch 22/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 1.8759 - mean_squared_error: 1.8759 - mean_absolute_error: 1.0885 - val_loss: 0.4142 - val_mean_squared_error: 0.4142 - val_mean_absolute_error: 0.4953\n",
      "Epoch 23/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.8856 - mean_squared_error: 1.8856 - mean_absolute_error: 1.0798 - val_loss: 0.4739 - val_mean_squared_error: 0.4739 - val_mean_absolute_error: 0.5414\n",
      "Epoch 24/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 1.7324 - mean_squared_error: 1.7324 - mean_absolute_error: 1.0420 - val_loss: 0.4448 - val_mean_squared_error: 0.4448 - val_mean_absolute_error: 0.5209\n",
      "Epoch 25/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.7485 - mean_squared_error: 1.7485 - mean_absolute_error: 1.0486 - val_loss: 0.3787 - val_mean_squared_error: 0.3787 - val_mean_absolute_error: 0.4680\n",
      "Epoch 26/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 1.6171 - mean_squared_error: 1.6171 - mean_absolute_error: 1.0029 - val_loss: 0.3630 - val_mean_squared_error: 0.3630 - val_mean_absolute_error: 0.4525\n",
      "Epoch 27/200\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 1.8153 - mean_squared_error: 1.8153 - mean_absolute_error: 1.0629 - val_loss: 0.3700 - val_mean_squared_error: 0.3700 - val_mean_absolute_error: 0.4586\n",
      "Epoch 28/200\n",
      "856/856 [==============================] - 0s 123us/sample - loss: 1.5346 - mean_squared_error: 1.5346 - mean_absolute_error: 0.9945 - val_loss: 0.4045 - val_mean_squared_error: 0.4045 - val_mean_absolute_error: 0.4831\n",
      "Epoch 29/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 1.6421 - mean_squared_error: 1.6421 - mean_absolute_error: 1.0216 - val_loss: 0.3425 - val_mean_squared_error: 0.3425 - val_mean_absolute_error: 0.4404\n",
      "Epoch 30/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 1.5707 - mean_squared_error: 1.5707 - mean_absolute_error: 0.9994 - val_loss: 0.3674 - val_mean_squared_error: 0.3674 - val_mean_absolute_error: 0.4549\n",
      "Epoch 31/200\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 1.6303 - mean_squared_error: 1.6303 - mean_absolute_error: 1.0119 - val_loss: 0.3780 - val_mean_squared_error: 0.3780 - val_mean_absolute_error: 0.4661\n",
      "Epoch 32/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 1.6272 - mean_squared_error: 1.6272 - mean_absolute_error: 1.0166 - val_loss: 0.3631 - val_mean_squared_error: 0.3631 - val_mean_absolute_error: 0.4534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 1.4783 - mean_squared_error: 1.4783 - mean_absolute_error: 0.9534 - val_loss: 0.3885 - val_mean_squared_error: 0.3885 - val_mean_absolute_error: 0.4712\n",
      "Epoch 34/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.5172 - mean_squared_error: 1.5172 - mean_absolute_error: 0.9754 - val_loss: 0.3558 - val_mean_squared_error: 0.3558 - val_mean_absolute_error: 0.4483\n",
      "Epoch 35/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.5423 - mean_squared_error: 1.5423 - mean_absolute_error: 0.9771 - val_loss: 0.4612 - val_mean_squared_error: 0.4612 - val_mean_absolute_error: 0.5194\n",
      "Epoch 36/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 1.3351 - mean_squared_error: 1.3351 - mean_absolute_error: 0.9081 - val_loss: 0.3669 - val_mean_squared_error: 0.3669 - val_mean_absolute_error: 0.4606\n",
      "Epoch 37/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 1.4763 - mean_squared_error: 1.4763 - mean_absolute_error: 0.9537 - val_loss: 0.3801 - val_mean_squared_error: 0.3801 - val_mean_absolute_error: 0.4656\n",
      "Epoch 38/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.4302 - mean_squared_error: 1.4302 - mean_absolute_error: 0.9583 - val_loss: 0.3915 - val_mean_squared_error: 0.3915 - val_mean_absolute_error: 0.4734\n",
      "Epoch 39/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 1.4122 - mean_squared_error: 1.4122 - mean_absolute_error: 0.9348 - val_loss: 0.3671 - val_mean_squared_error: 0.3671 - val_mean_absolute_error: 0.4559\n",
      "Epoch 40/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.5220 - mean_squared_error: 1.5220 - mean_absolute_error: 0.9690 - val_loss: 0.3487 - val_mean_squared_error: 0.3487 - val_mean_absolute_error: 0.4493\n",
      "Epoch 41/200\n",
      "856/856 [==============================] - 0s 123us/sample - loss: 1.5098 - mean_squared_error: 1.5098 - mean_absolute_error: 0.9669 - val_loss: 0.4152 - val_mean_squared_error: 0.4152 - val_mean_absolute_error: 0.4891\n",
      "Epoch 42/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.4179 - mean_squared_error: 1.4179 - mean_absolute_error: 0.9269 - val_loss: 0.3600 - val_mean_squared_error: 0.3600 - val_mean_absolute_error: 0.4576\n",
      "Epoch 43/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 1.4499 - mean_squared_error: 1.4499 - mean_absolute_error: 0.9471 - val_loss: 0.3894 - val_mean_squared_error: 0.3894 - val_mean_absolute_error: 0.4704\n",
      "Epoch 44/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.2891 - mean_squared_error: 1.2891 - mean_absolute_error: 0.8789 - val_loss: 0.3825 - val_mean_squared_error: 0.3825 - val_mean_absolute_error: 0.4656\n",
      "Epoch 45/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.2605 - mean_squared_error: 1.2605 - mean_absolute_error: 0.8978 - val_loss: 0.3645 - val_mean_squared_error: 0.3645 - val_mean_absolute_error: 0.4524\n",
      "Epoch 46/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 1.1854 - mean_squared_error: 1.1854 - mean_absolute_error: 0.8625 - val_loss: 0.3570 - val_mean_squared_error: 0.3570 - val_mean_absolute_error: 0.4481\n",
      "Epoch 47/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 1.3321 - mean_squared_error: 1.3321 - mean_absolute_error: 0.9079 - val_loss: 0.3471 - val_mean_squared_error: 0.3471 - val_mean_absolute_error: 0.4400\n",
      "Epoch 48/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 1.2612 - mean_squared_error: 1.2612 - mean_absolute_error: 0.8787 - val_loss: 0.3784 - val_mean_squared_error: 0.3784 - val_mean_absolute_error: 0.4642\n",
      "Epoch 49/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 1.2372 - mean_squared_error: 1.2372 - mean_absolute_error: 0.8758 - val_loss: 0.3944 - val_mean_squared_error: 0.3944 - val_mean_absolute_error: 0.4714\n",
      "Epoch 50/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.2090 - mean_squared_error: 1.2090 - mean_absolute_error: 0.8702 - val_loss: 0.3707 - val_mean_squared_error: 0.3707 - val_mean_absolute_error: 0.4557\n",
      "Epoch 51/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.2088 - mean_squared_error: 1.2088 - mean_absolute_error: 0.8489 - val_loss: 0.3457 - val_mean_squared_error: 0.3457 - val_mean_absolute_error: 0.4386\n",
      "Epoch 52/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 1.1942 - mean_squared_error: 1.1942 - mean_absolute_error: 0.8636 - val_loss: 0.3667 - val_mean_squared_error: 0.3667 - val_mean_absolute_error: 0.4495\n",
      "Epoch 53/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.2948 - mean_squared_error: 1.2948 - mean_absolute_error: 0.8831 - val_loss: 0.3744 - val_mean_squared_error: 0.3744 - val_mean_absolute_error: 0.4537\n",
      "Epoch 54/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 1.0681 - mean_squared_error: 1.0681 - mean_absolute_error: 0.8073 - val_loss: 0.3379 - val_mean_squared_error: 0.3379 - val_mean_absolute_error: 0.4318\n",
      "Epoch 55/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.1171 - mean_squared_error: 1.1171 - mean_absolute_error: 0.8287 - val_loss: 0.3557 - val_mean_squared_error: 0.3557 - val_mean_absolute_error: 0.4411\n",
      "Epoch 56/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 1.1934 - mean_squared_error: 1.1934 - mean_absolute_error: 0.8732 - val_loss: 0.3605 - val_mean_squared_error: 0.3605 - val_mean_absolute_error: 0.4515\n",
      "Epoch 57/200\n",
      "856/856 [==============================] - 0s 98us/sample - loss: 1.0923 - mean_squared_error: 1.0923 - mean_absolute_error: 0.8084 - val_loss: 0.3733 - val_mean_squared_error: 0.3733 - val_mean_absolute_error: 0.4553\n",
      "Epoch 58/200\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 1.0393 - mean_squared_error: 1.0393 - mean_absolute_error: 0.8142 - val_loss: 0.3453 - val_mean_squared_error: 0.3453 - val_mean_absolute_error: 0.4438\n",
      "Epoch 59/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.1199 - mean_squared_error: 1.1199 - mean_absolute_error: 0.8397 - val_loss: 0.3699 - val_mean_squared_error: 0.3699 - val_mean_absolute_error: 0.4539\n",
      "Epoch 60/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 1.1566 - mean_squared_error: 1.1566 - mean_absolute_error: 0.8406 - val_loss: 0.3507 - val_mean_squared_error: 0.3507 - val_mean_absolute_error: 0.4420\n",
      "Epoch 61/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.1099 - mean_squared_error: 1.1099 - mean_absolute_error: 0.8131 - val_loss: 0.3759 - val_mean_squared_error: 0.3759 - val_mean_absolute_error: 0.4583\n",
      "Epoch 62/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.0434 - mean_squared_error: 1.0434 - mean_absolute_error: 0.7960 - val_loss: 0.3615 - val_mean_squared_error: 0.3615 - val_mean_absolute_error: 0.4458\n",
      "Epoch 63/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 1.0836 - mean_squared_error: 1.0836 - mean_absolute_error: 0.8110 - val_loss: 0.3583 - val_mean_squared_error: 0.3583 - val_mean_absolute_error: 0.4429\n",
      "Epoch 64/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 1.0318 - mean_squared_error: 1.0318 - mean_absolute_error: 0.8035 - val_loss: 0.3611 - val_mean_squared_error: 0.3611 - val_mean_absolute_error: 0.4502\n",
      "Epoch 65/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 1.1423 - mean_squared_error: 1.1423 - mean_absolute_error: 0.8327 - val_loss: 0.3615 - val_mean_squared_error: 0.3615 - val_mean_absolute_error: 0.4505\n",
      "Epoch 66/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 1.0344 - mean_squared_error: 1.0344 - mean_absolute_error: 0.7962 - val_loss: 0.3506 - val_mean_squared_error: 0.3506 - val_mean_absolute_error: 0.4457\n",
      "Epoch 67/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 1.0232 - mean_squared_error: 1.0232 - mean_absolute_error: 0.8026 - val_loss: 0.3574 - val_mean_squared_error: 0.3574 - val_mean_absolute_error: 0.4423\n",
      "Epoch 68/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 106us/sample - loss: 1.1111 - mean_squared_error: 1.1111 - mean_absolute_error: 0.8298 - val_loss: 0.4097 - val_mean_squared_error: 0.4097 - val_mean_absolute_error: 0.4780\n",
      "Epoch 69/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.9720 - mean_squared_error: 0.9720 - mean_absolute_error: 0.7741 - val_loss: 0.3557 - val_mean_squared_error: 0.3557 - val_mean_absolute_error: 0.4596\n",
      "Epoch 70/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.0180 - mean_squared_error: 1.0180 - mean_absolute_error: 0.7963 - val_loss: 0.3817 - val_mean_squared_error: 0.3817 - val_mean_absolute_error: 0.4693\n",
      "Epoch 71/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.9335 - mean_squared_error: 0.9335 - mean_absolute_error: 0.7580 - val_loss: 0.3537 - val_mean_squared_error: 0.3537 - val_mean_absolute_error: 0.4538\n",
      "Epoch 72/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 1.0135 - mean_squared_error: 1.0135 - mean_absolute_error: 0.7957 - val_loss: 0.3837 - val_mean_squared_error: 0.3837 - val_mean_absolute_error: 0.4720\n",
      "Epoch 73/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.9457 - mean_squared_error: 0.9457 - mean_absolute_error: 0.7779 - val_loss: 0.3754 - val_mean_squared_error: 0.3754 - val_mean_absolute_error: 0.4636\n",
      "Epoch 74/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.9834 - mean_squared_error: 0.9834 - mean_absolute_error: 0.7813 - val_loss: 0.3631 - val_mean_squared_error: 0.3631 - val_mean_absolute_error: 0.4549\n",
      "Epoch 75/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 1.0278 - mean_squared_error: 1.0278 - mean_absolute_error: 0.7888 - val_loss: 0.3496 - val_mean_squared_error: 0.3496 - val_mean_absolute_error: 0.4573\n",
      "Epoch 76/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.9292 - mean_squared_error: 0.9292 - mean_absolute_error: 0.7583 - val_loss: 0.3741 - val_mean_squared_error: 0.3741 - val_mean_absolute_error: 0.4654\n",
      "Epoch 77/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.9702 - mean_squared_error: 0.9702 - mean_absolute_error: 0.7649 - val_loss: 0.3665 - val_mean_squared_error: 0.3665 - val_mean_absolute_error: 0.4624\n",
      "Epoch 78/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.9327 - mean_squared_error: 0.9327 - mean_absolute_error: 0.7618 - val_loss: 0.3565 - val_mean_squared_error: 0.3565 - val_mean_absolute_error: 0.4560\n",
      "Epoch 79/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.9601 - mean_squared_error: 0.9601 - mean_absolute_error: 0.7569 - val_loss: 0.3579 - val_mean_squared_error: 0.3579 - val_mean_absolute_error: 0.4571\n",
      "Epoch 80/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.9231 - mean_squared_error: 0.9231 - mean_absolute_error: 0.7594 - val_loss: 0.3870 - val_mean_squared_error: 0.3870 - val_mean_absolute_error: 0.4758\n",
      "Epoch 81/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.9579 - mean_squared_error: 0.9579 - mean_absolute_error: 0.7723 - val_loss: 0.3650 - val_mean_squared_error: 0.3650 - val_mean_absolute_error: 0.4608\n",
      "Epoch 82/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.9222 - mean_squared_error: 0.9222 - mean_absolute_error: 0.7633 - val_loss: 0.3648 - val_mean_squared_error: 0.3648 - val_mean_absolute_error: 0.4615\n",
      "Epoch 83/200\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.9022 - mean_squared_error: 0.9022 - mean_absolute_error: 0.7373 - val_loss: 0.3542 - val_mean_squared_error: 0.3542 - val_mean_absolute_error: 0.4595\n",
      "Epoch 84/200\n",
      "856/856 [==============================] - 0s 130us/sample - loss: 0.8986 - mean_squared_error: 0.8986 - mean_absolute_error: 0.7423 - val_loss: 0.3705 - val_mean_squared_error: 0.3705 - val_mean_absolute_error: 0.4659\n",
      "Epoch 85/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.8888 - mean_squared_error: 0.8888 - mean_absolute_error: 0.7508 - val_loss: 0.3555 - val_mean_squared_error: 0.3555 - val_mean_absolute_error: 0.4570\n",
      "Epoch 86/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.9086 - mean_squared_error: 0.9086 - mean_absolute_error: 0.7533 - val_loss: 0.3736 - val_mean_squared_error: 0.3736 - val_mean_absolute_error: 0.4723\n",
      "Epoch 87/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.8544 - mean_squared_error: 0.8544 - mean_absolute_error: 0.7271 - val_loss: 0.3493 - val_mean_squared_error: 0.3493 - val_mean_absolute_error: 0.4569\n",
      "Epoch 88/200\n",
      "856/856 [==============================] - 0s 98us/sample - loss: 0.9003 - mean_squared_error: 0.9003 - mean_absolute_error: 0.7374 - val_loss: 0.3798 - val_mean_squared_error: 0.3798 - val_mean_absolute_error: 0.4718\n",
      "Epoch 89/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.8253 - mean_squared_error: 0.8253 - mean_absolute_error: 0.7131 - val_loss: 0.3738 - val_mean_squared_error: 0.3738 - val_mean_absolute_error: 0.4637\n",
      "Epoch 90/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.8739 - mean_squared_error: 0.8739 - mean_absolute_error: 0.7254 - val_loss: 0.3617 - val_mean_squared_error: 0.3617 - val_mean_absolute_error: 0.4635\n",
      "Epoch 91/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.8049 - mean_squared_error: 0.8049 - mean_absolute_error: 0.6972 - val_loss: 0.3655 - val_mean_squared_error: 0.3655 - val_mean_absolute_error: 0.4610\n",
      "Epoch 92/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.8635 - mean_squared_error: 0.8635 - mean_absolute_error: 0.7230 - val_loss: 0.3508 - val_mean_squared_error: 0.3508 - val_mean_absolute_error: 0.4515\n",
      "Epoch 93/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.8313 - mean_squared_error: 0.8313 - mean_absolute_error: 0.7091 - val_loss: 0.3547 - val_mean_squared_error: 0.3547 - val_mean_absolute_error: 0.4571\n",
      "Epoch 94/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.8287 - mean_squared_error: 0.8287 - mean_absolute_error: 0.7107 - val_loss: 0.3715 - val_mean_squared_error: 0.3715 - val_mean_absolute_error: 0.4672\n",
      "Epoch 95/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.8849 - mean_squared_error: 0.8849 - mean_absolute_error: 0.7407 - val_loss: 0.3485 - val_mean_squared_error: 0.3485 - val_mean_absolute_error: 0.4551\n",
      "Epoch 96/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.8330 - mean_squared_error: 0.8330 - mean_absolute_error: 0.7059 - val_loss: 0.3450 - val_mean_squared_error: 0.3450 - val_mean_absolute_error: 0.4496\n",
      "Epoch 97/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.8204 - mean_squared_error: 0.8204 - mean_absolute_error: 0.7162 - val_loss: 0.3446 - val_mean_squared_error: 0.3446 - val_mean_absolute_error: 0.4527\n",
      "Epoch 98/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.7334 - mean_squared_error: 0.7334 - mean_absolute_error: 0.6790 - val_loss: 0.3553 - val_mean_squared_error: 0.3553 - val_mean_absolute_error: 0.4579\n",
      "Epoch 99/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.8093 - mean_squared_error: 0.8093 - mean_absolute_error: 0.7052 - val_loss: 0.3532 - val_mean_squared_error: 0.3532 - val_mean_absolute_error: 0.4586\n",
      "Epoch 100/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.7474 - mean_squared_error: 0.7474 - mean_absolute_error: 0.6813 - val_loss: 0.3532 - val_mean_squared_error: 0.3532 - val_mean_absolute_error: 0.4605\n",
      "Epoch 101/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.7558 - mean_squared_error: 0.7558 - mean_absolute_error: 0.6773 - val_loss: 0.3592 - val_mean_squared_error: 0.3592 - val_mean_absolute_error: 0.4553\n",
      "Epoch 102/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.7997 - mean_squared_error: 0.7997 - mean_absolute_error: 0.6858 - val_loss: 0.3485 - val_mean_squared_error: 0.3485 - val_mean_absolute_error: 0.4566\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 103us/sample - loss: 0.7507 - mean_squared_error: 0.7507 - mean_absolute_error: 0.6734 - val_loss: 0.3521 - val_mean_squared_error: 0.3521 - val_mean_absolute_error: 0.4580\n",
      "Epoch 104/200\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.7875 - mean_squared_error: 0.7875 - mean_absolute_error: 0.6932 - val_loss: 0.3737 - val_mean_squared_error: 0.3737 - val_mean_absolute_error: 0.4725\n",
      "Epoch 105/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.8097 - mean_squared_error: 0.8097 - mean_absolute_error: 0.6896 - val_loss: 0.3629 - val_mean_squared_error: 0.3629 - val_mean_absolute_error: 0.4652\n",
      "Epoch 106/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.7340 - mean_squared_error: 0.7340 - mean_absolute_error: 0.6666 - val_loss: 0.3669 - val_mean_squared_error: 0.3669 - val_mean_absolute_error: 0.4723\n",
      "Epoch 107/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.7257 - mean_squared_error: 0.7257 - mean_absolute_error: 0.6671 - val_loss: 0.3631 - val_mean_squared_error: 0.3631 - val_mean_absolute_error: 0.4638\n",
      "Epoch 108/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.7052 - mean_squared_error: 0.7052 - mean_absolute_error: 0.6552 - val_loss: 0.3621 - val_mean_squared_error: 0.3621 - val_mean_absolute_error: 0.4629\n",
      "Epoch 109/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.7029 - mean_squared_error: 0.7029 - mean_absolute_error: 0.6451 - val_loss: 0.3518 - val_mean_squared_error: 0.3518 - val_mean_absolute_error: 0.4596\n",
      "Epoch 110/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.7348 - mean_squared_error: 0.7348 - mean_absolute_error: 0.6782 - val_loss: 0.3477 - val_mean_squared_error: 0.3477 - val_mean_absolute_error: 0.4536\n",
      "Epoch 111/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.6956 - mean_squared_error: 0.6956 - mean_absolute_error: 0.6605 - val_loss: 0.3469 - val_mean_squared_error: 0.3469 - val_mean_absolute_error: 0.4553\n",
      "Epoch 112/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.6823 - mean_squared_error: 0.6823 - mean_absolute_error: 0.6467 - val_loss: 0.3579 - val_mean_squared_error: 0.3579 - val_mean_absolute_error: 0.4630\n",
      "Epoch 113/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.6969 - mean_squared_error: 0.6969 - mean_absolute_error: 0.6497 - val_loss: 0.3582 - val_mean_squared_error: 0.3582 - val_mean_absolute_error: 0.4682\n",
      "Epoch 114/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.6783 - mean_squared_error: 0.6783 - mean_absolute_error: 0.6436 - val_loss: 0.3671 - val_mean_squared_error: 0.3671 - val_mean_absolute_error: 0.4662\n",
      "Epoch 115/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.7005 - mean_squared_error: 0.7005 - mean_absolute_error: 0.6480 - val_loss: 0.3681 - val_mean_squared_error: 0.3681 - val_mean_absolute_error: 0.4636\n",
      "Epoch 116/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.7418 - mean_squared_error: 0.7418 - mean_absolute_error: 0.6669 - val_loss: 0.3653 - val_mean_squared_error: 0.3653 - val_mean_absolute_error: 0.4621\n",
      "Epoch 117/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.7411 - mean_squared_error: 0.7411 - mean_absolute_error: 0.6526 - val_loss: 0.3578 - val_mean_squared_error: 0.3578 - val_mean_absolute_error: 0.4626\n",
      "Epoch 118/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.7122 - mean_squared_error: 0.7122 - mean_absolute_error: 0.6555 - val_loss: 0.3627 - val_mean_squared_error: 0.3627 - val_mean_absolute_error: 0.4643\n",
      "Epoch 119/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.6235 - mean_squared_error: 0.6235 - mean_absolute_error: 0.6183 - val_loss: 0.3753 - val_mean_squared_error: 0.3753 - val_mean_absolute_error: 0.4734\n",
      "Epoch 120/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.6840 - mean_squared_error: 0.6840 - mean_absolute_error: 0.6390 - val_loss: 0.3563 - val_mean_squared_error: 0.3563 - val_mean_absolute_error: 0.4629\n",
      "Epoch 121/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.6679 - mean_squared_error: 0.6679 - mean_absolute_error: 0.6276 - val_loss: 0.3545 - val_mean_squared_error: 0.3545 - val_mean_absolute_error: 0.4604\n",
      "Epoch 122/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.6604 - mean_squared_error: 0.6604 - mean_absolute_error: 0.6257 - val_loss: 0.3583 - val_mean_squared_error: 0.3583 - val_mean_absolute_error: 0.4602\n",
      "Epoch 123/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.6678 - mean_squared_error: 0.6678 - mean_absolute_error: 0.6235 - val_loss: 0.3668 - val_mean_squared_error: 0.3668 - val_mean_absolute_error: 0.4632\n",
      "Epoch 124/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.6735 - mean_squared_error: 0.6735 - mean_absolute_error: 0.6408 - val_loss: 0.3643 - val_mean_squared_error: 0.3643 - val_mean_absolute_error: 0.4649\n",
      "Epoch 125/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.6393 - mean_squared_error: 0.6393 - mean_absolute_error: 0.6203 - val_loss: 0.3581 - val_mean_squared_error: 0.3581 - val_mean_absolute_error: 0.4630\n",
      "Epoch 126/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.6044 - mean_squared_error: 0.6044 - mean_absolute_error: 0.6085 - val_loss: 0.3578 - val_mean_squared_error: 0.3578 - val_mean_absolute_error: 0.4609\n",
      "Epoch 127/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.6777 - mean_squared_error: 0.6777 - mean_absolute_error: 0.6363 - val_loss: 0.3582 - val_mean_squared_error: 0.3582 - val_mean_absolute_error: 0.4605\n",
      "Epoch 128/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.6619 - mean_squared_error: 0.6619 - mean_absolute_error: 0.6297 - val_loss: 0.3536 - val_mean_squared_error: 0.3536 - val_mean_absolute_error: 0.4601\n",
      "Epoch 129/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.5898 - mean_squared_error: 0.5898 - mean_absolute_error: 0.5794 - val_loss: 0.3557 - val_mean_squared_error: 0.3557 - val_mean_absolute_error: 0.4637\n",
      "Epoch 130/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.6691 - mean_squared_error: 0.6691 - mean_absolute_error: 0.6264 - val_loss: 0.3681 - val_mean_squared_error: 0.3681 - val_mean_absolute_error: 0.4692\n",
      "Epoch 131/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.6260 - mean_squared_error: 0.6260 - mean_absolute_error: 0.6071 - val_loss: 0.3670 - val_mean_squared_error: 0.3670 - val_mean_absolute_error: 0.4681\n",
      "Epoch 132/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.6165 - mean_squared_error: 0.6165 - mean_absolute_error: 0.6170 - val_loss: 0.3442 - val_mean_squared_error: 0.3442 - val_mean_absolute_error: 0.4540\n",
      "Epoch 133/200\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.6485 - mean_squared_error: 0.6485 - mean_absolute_error: 0.6232 - val_loss: 0.3735 - val_mean_squared_error: 0.3735 - val_mean_absolute_error: 0.4689\n",
      "Epoch 134/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5960 - mean_squared_error: 0.5960 - mean_absolute_error: 0.6005 - val_loss: 0.3435 - val_mean_squared_error: 0.3435 - val_mean_absolute_error: 0.4556\n",
      "Epoch 135/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.5783 - mean_squared_error: 0.5783 - mean_absolute_error: 0.5852 - val_loss: 0.3483 - val_mean_squared_error: 0.3483 - val_mean_absolute_error: 0.4540\n",
      "Epoch 136/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.5967 - mean_squared_error: 0.5967 - mean_absolute_error: 0.5952 - val_loss: 0.3631 - val_mean_squared_error: 0.3631 - val_mean_absolute_error: 0.4685\n",
      "Epoch 137/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.6204 - mean_squared_error: 0.6204 - mean_absolute_error: 0.6208 - val_loss: 0.3519 - val_mean_squared_error: 0.3519 - val_mean_absolute_error: 0.4606\n",
      "Epoch 138/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 105us/sample - loss: 0.6507 - mean_squared_error: 0.6507 - mean_absolute_error: 0.6264 - val_loss: 0.3721 - val_mean_squared_error: 0.3721 - val_mean_absolute_error: 0.4720\n",
      "Epoch 139/200\n",
      "856/856 [==============================] - 0s 127us/sample - loss: 0.6376 - mean_squared_error: 0.6376 - mean_absolute_error: 0.6171 - val_loss: 0.3581 - val_mean_squared_error: 0.3581 - val_mean_absolute_error: 0.4632\n",
      "Epoch 140/200\n",
      "856/856 [==============================] - 0s 129us/sample - loss: 0.5711 - mean_squared_error: 0.5711 - mean_absolute_error: 0.5867 - val_loss: 0.3503 - val_mean_squared_error: 0.3503 - val_mean_absolute_error: 0.4567\n",
      "Epoch 141/200\n",
      "856/856 [==============================] - 0s 97us/sample - loss: 0.5951 - mean_squared_error: 0.5951 - mean_absolute_error: 0.5972 - val_loss: 0.3591 - val_mean_squared_error: 0.3591 - val_mean_absolute_error: 0.4641\n",
      "Epoch 142/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.5924 - mean_squared_error: 0.5924 - mean_absolute_error: 0.5986 - val_loss: 0.3491 - val_mean_squared_error: 0.3491 - val_mean_absolute_error: 0.4602\n",
      "Epoch 143/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.6271 - mean_squared_error: 0.6271 - mean_absolute_error: 0.6144 - val_loss: 0.3657 - val_mean_squared_error: 0.3657 - val_mean_absolute_error: 0.4642\n",
      "Epoch 144/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5907 - mean_squared_error: 0.5907 - mean_absolute_error: 0.6003 - val_loss: 0.3453 - val_mean_squared_error: 0.3453 - val_mean_absolute_error: 0.4547\n",
      "Epoch 145/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.6122 - mean_squared_error: 0.6122 - mean_absolute_error: 0.6022 - val_loss: 0.3518 - val_mean_squared_error: 0.3518 - val_mean_absolute_error: 0.4581\n",
      "Epoch 146/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5896 - mean_squared_error: 0.5896 - mean_absolute_error: 0.5987 - val_loss: 0.3488 - val_mean_squared_error: 0.3488 - val_mean_absolute_error: 0.4589\n",
      "Epoch 147/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5974 - mean_squared_error: 0.5974 - mean_absolute_error: 0.5988 - val_loss: 0.3527 - val_mean_squared_error: 0.3527 - val_mean_absolute_error: 0.4620\n",
      "Epoch 148/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.5590 - mean_squared_error: 0.5590 - mean_absolute_error: 0.5785 - val_loss: 0.3444 - val_mean_squared_error: 0.3444 - val_mean_absolute_error: 0.4558\n",
      "Epoch 149/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5531 - mean_squared_error: 0.5531 - mean_absolute_error: 0.5742 - val_loss: 0.3446 - val_mean_squared_error: 0.3446 - val_mean_absolute_error: 0.4555\n",
      "Epoch 150/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.5639 - mean_squared_error: 0.5639 - mean_absolute_error: 0.5782 - val_loss: 0.3520 - val_mean_squared_error: 0.3520 - val_mean_absolute_error: 0.4570\n",
      "Epoch 151/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5520 - mean_squared_error: 0.5520 - mean_absolute_error: 0.5756 - val_loss: 0.3657 - val_mean_squared_error: 0.3657 - val_mean_absolute_error: 0.4669\n",
      "Epoch 152/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5648 - mean_squared_error: 0.5648 - mean_absolute_error: 0.5786 - val_loss: 0.3461 - val_mean_squared_error: 0.3461 - val_mean_absolute_error: 0.4552\n",
      "Epoch 153/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5554 - mean_squared_error: 0.5554 - mean_absolute_error: 0.5826 - val_loss: 0.3546 - val_mean_squared_error: 0.3546 - val_mean_absolute_error: 0.4598\n",
      "Epoch 154/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.5403 - mean_squared_error: 0.5403 - mean_absolute_error: 0.5688 - val_loss: 0.3474 - val_mean_squared_error: 0.3474 - val_mean_absolute_error: 0.4583\n",
      "Epoch 155/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.5715 - mean_squared_error: 0.5715 - mean_absolute_error: 0.5785 - val_loss: 0.3624 - val_mean_squared_error: 0.3624 - val_mean_absolute_error: 0.4638\n",
      "Epoch 156/200\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.5647 - mean_squared_error: 0.5647 - mean_absolute_error: 0.5800 - val_loss: 0.3617 - val_mean_squared_error: 0.3617 - val_mean_absolute_error: 0.4661\n",
      "Epoch 157/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.5346 - mean_squared_error: 0.5346 - mean_absolute_error: 0.5697 - val_loss: 0.3564 - val_mean_squared_error: 0.3564 - val_mean_absolute_error: 0.4615\n",
      "Epoch 158/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.5543 - mean_squared_error: 0.5543 - mean_absolute_error: 0.5735 - val_loss: 0.3463 - val_mean_squared_error: 0.3463 - val_mean_absolute_error: 0.4545\n",
      "Epoch 159/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.5355 - mean_squared_error: 0.5355 - mean_absolute_error: 0.5671 - val_loss: 0.3519 - val_mean_squared_error: 0.3519 - val_mean_absolute_error: 0.4612\n",
      "Epoch 160/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.5596 - mean_squared_error: 0.5596 - mean_absolute_error: 0.5760 - val_loss: 0.3550 - val_mean_squared_error: 0.3550 - val_mean_absolute_error: 0.4668\n",
      "Epoch 161/200\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.5430 - mean_squared_error: 0.5430 - mean_absolute_error: 0.5765 - val_loss: 0.3512 - val_mean_squared_error: 0.3512 - val_mean_absolute_error: 0.4613\n",
      "Epoch 162/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.5397 - mean_squared_error: 0.5397 - mean_absolute_error: 0.5646 - val_loss: 0.3626 - val_mean_squared_error: 0.3626 - val_mean_absolute_error: 0.4695\n",
      "Epoch 163/200\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.5546 - mean_squared_error: 0.5546 - mean_absolute_error: 0.5762 - val_loss: 0.3576 - val_mean_squared_error: 0.3576 - val_mean_absolute_error: 0.4649\n",
      "Epoch 164/200\n",
      "856/856 [==============================] - 0s 99us/sample - loss: 0.5340 - mean_squared_error: 0.5340 - mean_absolute_error: 0.5702 - val_loss: 0.3519 - val_mean_squared_error: 0.3519 - val_mean_absolute_error: 0.4632\n",
      "Epoch 165/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.5227 - mean_squared_error: 0.5227 - mean_absolute_error: 0.5579 - val_loss: 0.3572 - val_mean_squared_error: 0.3572 - val_mean_absolute_error: 0.4662\n",
      "Epoch 166/200\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.5325 - mean_squared_error: 0.5325 - mean_absolute_error: 0.5590 - val_loss: 0.3489 - val_mean_squared_error: 0.3489 - val_mean_absolute_error: 0.4609\n",
      "Epoch 167/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5551 - mean_squared_error: 0.5551 - mean_absolute_error: 0.5702 - val_loss: 0.3593 - val_mean_squared_error: 0.3593 - val_mean_absolute_error: 0.4686\n",
      "Epoch 168/200\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.5003 - mean_squared_error: 0.5003 - mean_absolute_error: 0.5551 - val_loss: 0.3541 - val_mean_squared_error: 0.3541 - val_mean_absolute_error: 0.4631\n",
      "Epoch 169/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.5411 - mean_squared_error: 0.5411 - mean_absolute_error: 0.5659 - val_loss: 0.3518 - val_mean_squared_error: 0.3518 - val_mean_absolute_error: 0.4642\n",
      "Epoch 170/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.5022 - mean_squared_error: 0.5022 - mean_absolute_error: 0.5613 - val_loss: 0.3453 - val_mean_squared_error: 0.3453 - val_mean_absolute_error: 0.4593\n",
      "Epoch 171/200\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.5036 - mean_squared_error: 0.5036 - mean_absolute_error: 0.5514 - val_loss: 0.3511 - val_mean_squared_error: 0.3511 - val_mean_absolute_error: 0.4625\n",
      "Epoch 172/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5372 - mean_squared_error: 0.5372 - mean_absolute_error: 0.5677 - val_loss: 0.3485 - val_mean_squared_error: 0.3485 - val_mean_absolute_error: 0.4596\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5192 - mean_squared_error: 0.5192 - mean_absolute_error: 0.5539 - val_loss: 0.3502 - val_mean_squared_error: 0.3502 - val_mean_absolute_error: 0.4605\n",
      "Epoch 174/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5241 - mean_squared_error: 0.5241 - mean_absolute_error: 0.5595 - val_loss: 0.3574 - val_mean_squared_error: 0.3574 - val_mean_absolute_error: 0.4653\n",
      "Epoch 175/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.5277 - mean_squared_error: 0.5277 - mean_absolute_error: 0.5591 - val_loss: 0.3495 - val_mean_squared_error: 0.3495 - val_mean_absolute_error: 0.4626\n",
      "Epoch 176/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4955 - mean_squared_error: 0.4955 - mean_absolute_error: 0.5476 - val_loss: 0.3533 - val_mean_squared_error: 0.3533 - val_mean_absolute_error: 0.4624\n",
      "Epoch 177/200\n",
      "856/856 [==============================] - 0s 100us/sample - loss: 0.5281 - mean_squared_error: 0.5281 - mean_absolute_error: 0.5624 - val_loss: 0.3483 - val_mean_squared_error: 0.3483 - val_mean_absolute_error: 0.4593\n",
      "Epoch 178/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5429 - mean_squared_error: 0.5429 - mean_absolute_error: 0.5695 - val_loss: 0.3561 - val_mean_squared_error: 0.3561 - val_mean_absolute_error: 0.4638\n",
      "Epoch 179/200\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.5131 - mean_squared_error: 0.5131 - mean_absolute_error: 0.5566 - val_loss: 0.3579 - val_mean_squared_error: 0.3579 - val_mean_absolute_error: 0.4645\n",
      "Epoch 180/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.5269 - mean_squared_error: 0.5269 - mean_absolute_error: 0.5603 - val_loss: 0.3511 - val_mean_squared_error: 0.3511 - val_mean_absolute_error: 0.4611\n",
      "Epoch 181/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5181 - mean_squared_error: 0.5181 - mean_absolute_error: 0.5512 - val_loss: 0.3501 - val_mean_squared_error: 0.3501 - val_mean_absolute_error: 0.4621\n",
      "Epoch 182/200\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.5112 - mean_squared_error: 0.5112 - mean_absolute_error: 0.5538 - val_loss: 0.3535 - val_mean_squared_error: 0.3535 - val_mean_absolute_error: 0.4620\n",
      "Epoch 183/200\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.5182 - mean_squared_error: 0.5182 - mean_absolute_error: 0.5592 - val_loss: 0.3570 - val_mean_squared_error: 0.3570 - val_mean_absolute_error: 0.4643\n",
      "Epoch 184/200\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.5243 - mean_squared_error: 0.5243 - mean_absolute_error: 0.5637 - val_loss: 0.3480 - val_mean_squared_error: 0.3480 - val_mean_absolute_error: 0.4582\n",
      "Epoch 185/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.5013 - mean_squared_error: 0.5013 - mean_absolute_error: 0.5462 - val_loss: 0.3506 - val_mean_squared_error: 0.3506 - val_mean_absolute_error: 0.4611\n",
      "Epoch 186/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.5339 - mean_squared_error: 0.5339 - mean_absolute_error: 0.5586 - val_loss: 0.3563 - val_mean_squared_error: 0.3563 - val_mean_absolute_error: 0.4633\n",
      "Epoch 187/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.5067 - mean_squared_error: 0.5067 - mean_absolute_error: 0.5489 - val_loss: 0.3527 - val_mean_squared_error: 0.3527 - val_mean_absolute_error: 0.4638\n",
      "Epoch 188/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.5091 - mean_squared_error: 0.5091 - mean_absolute_error: 0.5617 - val_loss: 0.3506 - val_mean_squared_error: 0.3506 - val_mean_absolute_error: 0.4606\n",
      "Epoch 189/200\n",
      "856/856 [==============================] - 0s 120us/sample - loss: 0.5444 - mean_squared_error: 0.5444 - mean_absolute_error: 0.5661 - val_loss: 0.3584 - val_mean_squared_error: 0.3584 - val_mean_absolute_error: 0.4657\n",
      "Epoch 190/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.5177 - mean_squared_error: 0.5177 - mean_absolute_error: 0.5592 - val_loss: 0.3522 - val_mean_squared_error: 0.3522 - val_mean_absolute_error: 0.4606\n",
      "Epoch 191/200\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.5026 - mean_squared_error: 0.5026 - mean_absolute_error: 0.5447 - val_loss: 0.3515 - val_mean_squared_error: 0.3515 - val_mean_absolute_error: 0.4599\n",
      "Epoch 192/200\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.5222 - mean_squared_error: 0.5222 - mean_absolute_error: 0.5601 - val_loss: 0.3481 - val_mean_squared_error: 0.3481 - val_mean_absolute_error: 0.4583\n",
      "Epoch 193/200\n",
      "856/856 [==============================] - 0s 126us/sample - loss: 0.5067 - mean_squared_error: 0.5067 - mean_absolute_error: 0.5482 - val_loss: 0.3576 - val_mean_squared_error: 0.3576 - val_mean_absolute_error: 0.4672\n",
      "Epoch 194/200\n",
      "856/856 [==============================] - 0s 182us/sample - loss: 0.4971 - mean_squared_error: 0.4971 - mean_absolute_error: 0.5514 - val_loss: 0.3543 - val_mean_squared_error: 0.3543 - val_mean_absolute_error: 0.4644\n",
      "Epoch 195/200\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.5020 - mean_squared_error: 0.5020 - mean_absolute_error: 0.5501 - val_loss: 0.3490 - val_mean_squared_error: 0.3490 - val_mean_absolute_error: 0.4598\n",
      "Epoch 196/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4848 - mean_squared_error: 0.4848 - mean_absolute_error: 0.5457 - val_loss: 0.3460 - val_mean_squared_error: 0.3460 - val_mean_absolute_error: 0.4579\n",
      "Epoch 197/200\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4946 - mean_squared_error: 0.4946 - mean_absolute_error: 0.5473 - val_loss: 0.3457 - val_mean_squared_error: 0.3457 - val_mean_absolute_error: 0.4579\n",
      "Epoch 198/200\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4815 - mean_squared_error: 0.4815 - mean_absolute_error: 0.5362 - val_loss: 0.3417 - val_mean_squared_error: 0.3417 - val_mean_absolute_error: 0.4546\n",
      "Epoch 199/200\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4984 - mean_squared_error: 0.4984 - mean_absolute_error: 0.5479 - val_loss: 0.3425 - val_mean_squared_error: 0.3425 - val_mean_absolute_error: 0.4547\n",
      "Epoch 200/200\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4576 - mean_squared_error: 0.4576 - mean_absolute_error: 0.5284 - val_loss: 0.3427 - val_mean_squared_error: 0.3427 - val_mean_absolute_error: 0.4538\n",
      "Train on 856 samples, validate on 215 samples\n",
      "Epoch 1/150\n",
      "856/856 [==============================] - 0s 329us/sample - loss: 0.9841 - acc: 0.7699 - val_loss: 0.6289 - val_acc: 0.8326\n",
      "Epoch 2/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.6250 - acc: 0.8294 - val_loss: 0.5236 - val_acc: 0.8326\n",
      "Epoch 3/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.5507 - acc: 0.8318 - val_loss: 0.4880 - val_acc: 0.8465\n",
      "Epoch 4/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.5076 - acc: 0.8376 - val_loss: 0.4626 - val_acc: 0.8512\n",
      "Epoch 5/150\n",
      "856/856 [==============================] - 0s 123us/sample - loss: 0.5043 - acc: 0.8294 - val_loss: 0.4445 - val_acc: 0.8558\n",
      "Epoch 6/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.5128 - acc: 0.8329 - val_loss: 0.4456 - val_acc: 0.8465\n",
      "Epoch 7/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4855 - acc: 0.8318 - val_loss: 0.4349 - val_acc: 0.8512\n",
      "Epoch 8/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4815 - acc: 0.8388 - val_loss: 0.4190 - val_acc: 0.8605\n",
      "Epoch 9/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4962 - acc: 0.8329 - val_loss: 0.4212 - val_acc: 0.8558\n",
      "Epoch 10/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4807 - acc: 0.8423 - val_loss: 0.4155 - val_acc: 0.8512\n",
      "Epoch 11/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4732 - acc: 0.8329 - val_loss: 0.4123 - val_acc: 0.8558\n",
      "Epoch 12/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4699 - acc: 0.8470 - val_loss: 0.4069 - val_acc: 0.8465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/150\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.4558 - acc: 0.8493 - val_loss: 0.4115 - val_acc: 0.8465\n",
      "Epoch 14/150\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.4648 - acc: 0.8341 - val_loss: 0.4071 - val_acc: 0.8372\n",
      "Epoch 15/150\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.4632 - acc: 0.8376 - val_loss: 0.4039 - val_acc: 0.8558\n",
      "Epoch 16/150\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.4582 - acc: 0.8446 - val_loss: 0.4094 - val_acc: 0.8465\n",
      "Epoch 17/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4589 - acc: 0.8411 - val_loss: 0.4053 - val_acc: 0.8465\n",
      "Epoch 18/150\n",
      "856/856 [==============================] - 0s 103us/sample - loss: 0.4647 - acc: 0.8400 - val_loss: 0.4047 - val_acc: 0.8512\n",
      "Epoch 19/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4489 - acc: 0.8470 - val_loss: 0.4071 - val_acc: 0.8372\n",
      "Epoch 20/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4524 - acc: 0.8435 - val_loss: 0.3997 - val_acc: 0.8512\n",
      "Epoch 21/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4612 - acc: 0.8364 - val_loss: 0.4001 - val_acc: 0.8605\n",
      "Epoch 22/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4378 - acc: 0.8458 - val_loss: 0.4040 - val_acc: 0.8372\n",
      "Epoch 23/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4390 - acc: 0.8458 - val_loss: 0.4013 - val_acc: 0.8419\n",
      "Epoch 24/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4311 - acc: 0.8481 - val_loss: 0.4001 - val_acc: 0.8326\n",
      "Epoch 25/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4310 - acc: 0.8551 - val_loss: 0.3986 - val_acc: 0.8558\n",
      "Epoch 26/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4644 - acc: 0.8376 - val_loss: 0.4026 - val_acc: 0.8558\n",
      "Epoch 27/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4445 - acc: 0.8388 - val_loss: 0.4028 - val_acc: 0.8465\n",
      "Epoch 28/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4403 - acc: 0.8423 - val_loss: 0.4054 - val_acc: 0.8372\n",
      "Epoch 29/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4439 - acc: 0.8376 - val_loss: 0.4065 - val_acc: 0.8465\n",
      "Epoch 30/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4558 - acc: 0.8353 - val_loss: 0.4048 - val_acc: 0.8465\n",
      "Epoch 31/150\n",
      "856/856 [==============================] - 0s 117us/sample - loss: 0.4340 - acc: 0.8470 - val_loss: 0.4018 - val_acc: 0.8419\n",
      "Epoch 32/150\n",
      "856/856 [==============================] - 0s 117us/sample - loss: 0.4339 - acc: 0.8540 - val_loss: 0.3984 - val_acc: 0.8372\n",
      "Epoch 33/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4420 - acc: 0.8446 - val_loss: 0.4036 - val_acc: 0.8372\n",
      "Epoch 34/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4201 - acc: 0.8516 - val_loss: 0.3987 - val_acc: 0.8419\n",
      "Epoch 35/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4221 - acc: 0.8388 - val_loss: 0.3982 - val_acc: 0.8512\n",
      "Epoch 36/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4173 - acc: 0.8423 - val_loss: 0.3963 - val_acc: 0.8558\n",
      "Epoch 37/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4313 - acc: 0.8446 - val_loss: 0.3999 - val_acc: 0.8512\n",
      "Epoch 38/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4395 - acc: 0.8481 - val_loss: 0.4047 - val_acc: 0.8419\n",
      "Epoch 39/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4244 - acc: 0.8481 - val_loss: 0.4057 - val_acc: 0.8512\n",
      "Epoch 40/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4194 - acc: 0.8446 - val_loss: 0.4008 - val_acc: 0.8465\n",
      "Epoch 41/150\n",
      "856/856 [==============================] - 0s 125us/sample - loss: 0.4291 - acc: 0.8446 - val_loss: 0.4008 - val_acc: 0.8465\n",
      "Epoch 42/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4239 - acc: 0.8551 - val_loss: 0.4007 - val_acc: 0.8419\n",
      "Epoch 43/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4284 - acc: 0.8423 - val_loss: 0.4004 - val_acc: 0.8419\n",
      "Epoch 44/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4110 - acc: 0.8575 - val_loss: 0.4000 - val_acc: 0.8419\n",
      "Epoch 45/150\n",
      "856/856 [==============================] - 0s 122us/sample - loss: 0.4454 - acc: 0.8411 - val_loss: 0.4040 - val_acc: 0.8465\n",
      "Epoch 46/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4338 - acc: 0.8411 - val_loss: 0.4060 - val_acc: 0.8465\n",
      "Epoch 47/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4316 - acc: 0.8481 - val_loss: 0.4013 - val_acc: 0.8465\n",
      "Epoch 48/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4495 - acc: 0.8411 - val_loss: 0.4064 - val_acc: 0.8465\n",
      "Epoch 49/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4068 - acc: 0.8586 - val_loss: 0.3996 - val_acc: 0.8419\n",
      "Epoch 50/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4229 - acc: 0.8458 - val_loss: 0.3987 - val_acc: 0.8465\n",
      "Epoch 51/150\n",
      "856/856 [==============================] - 0s 118us/sample - loss: 0.4305 - acc: 0.8423 - val_loss: 0.3998 - val_acc: 0.8326\n",
      "Epoch 52/150\n",
      "856/856 [==============================] - 0s 122us/sample - loss: 0.4273 - acc: 0.8481 - val_loss: 0.4005 - val_acc: 0.8465\n",
      "Epoch 53/150\n",
      "856/856 [==============================] - 0s 118us/sample - loss: 0.4273 - acc: 0.8516 - val_loss: 0.4013 - val_acc: 0.8419\n",
      "Epoch 54/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4120 - acc: 0.8516 - val_loss: 0.3989 - val_acc: 0.8558\n",
      "Epoch 55/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.4188 - acc: 0.8435 - val_loss: 0.3963 - val_acc: 0.8605\n",
      "Epoch 56/150\n",
      "856/856 [==============================] - 0s 120us/sample - loss: 0.4188 - acc: 0.8411 - val_loss: 0.3929 - val_acc: 0.8465\n",
      "Epoch 57/150\n",
      "856/856 [==============================] - 0s 138us/sample - loss: 0.4216 - acc: 0.8435 - val_loss: 0.3961 - val_acc: 0.8465\n",
      "Epoch 58/150\n",
      "856/856 [==============================] - 0s 144us/sample - loss: 0.4150 - acc: 0.8528 - val_loss: 0.3939 - val_acc: 0.8372\n",
      "Epoch 59/150\n",
      "856/856 [==============================] - 0s 143us/sample - loss: 0.4175 - acc: 0.8481 - val_loss: 0.3953 - val_acc: 0.8465\n",
      "Epoch 60/150\n",
      "856/856 [==============================] - 0s 166us/sample - loss: 0.4134 - acc: 0.8481 - val_loss: 0.3966 - val_acc: 0.8419\n",
      "Epoch 61/150\n",
      "856/856 [==============================] - 0s 138us/sample - loss: 0.4248 - acc: 0.8446 - val_loss: 0.3991 - val_acc: 0.8465\n",
      "Epoch 62/150\n",
      "856/856 [==============================] - 0s 175us/sample - loss: 0.4016 - acc: 0.8493 - val_loss: 0.3944 - val_acc: 0.8372\n",
      "Epoch 63/150\n",
      "856/856 [==============================] - 0s 127us/sample - loss: 0.4232 - acc: 0.8423 - val_loss: 0.3933 - val_acc: 0.8465\n",
      "Epoch 64/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4169 - acc: 0.8458 - val_loss: 0.3909 - val_acc: 0.8419\n",
      "Epoch 65/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4054 - acc: 0.8586 - val_loss: 0.3911 - val_acc: 0.8558\n",
      "Epoch 66/150\n",
      "856/856 [==============================] - 0s 117us/sample - loss: 0.4094 - acc: 0.8481 - val_loss: 0.3989 - val_acc: 0.8419\n",
      "Epoch 67/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4140 - acc: 0.8516 - val_loss: 0.3934 - val_acc: 0.8372\n",
      "Epoch 68/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.4121 - acc: 0.8481 - val_loss: 0.3939 - val_acc: 0.8558\n",
      "Epoch 69/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4069 - acc: 0.8458 - val_loss: 0.3912 - val_acc: 0.8558\n",
      "Epoch 70/150\n",
      "856/856 [==============================] - 0s 119us/sample - loss: 0.3941 - acc: 0.8551 - val_loss: 0.3917 - val_acc: 0.8558\n",
      "Epoch 71/150\n",
      "856/856 [==============================] - 0s 125us/sample - loss: 0.3971 - acc: 0.8470 - val_loss: 0.3913 - val_acc: 0.8512\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 112us/sample - loss: 0.3979 - acc: 0.8540 - val_loss: 0.3927 - val_acc: 0.8512\n",
      "Epoch 73/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4241 - acc: 0.8645 - val_loss: 0.3954 - val_acc: 0.8512\n",
      "Epoch 74/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4113 - acc: 0.8505 - val_loss: 0.3905 - val_acc: 0.8465\n",
      "Epoch 75/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.3959 - acc: 0.8540 - val_loss: 0.3902 - val_acc: 0.8512\n",
      "Epoch 76/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.4194 - acc: 0.8540 - val_loss: 0.3870 - val_acc: 0.8558\n",
      "Epoch 77/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4079 - acc: 0.8493 - val_loss: 0.3901 - val_acc: 0.8558\n",
      "Epoch 78/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4037 - acc: 0.8493 - val_loss: 0.3883 - val_acc: 0.8465\n",
      "Epoch 79/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3920 - acc: 0.8516 - val_loss: 0.3851 - val_acc: 0.8512\n",
      "Epoch 80/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.4050 - acc: 0.8540 - val_loss: 0.3853 - val_acc: 0.8558\n",
      "Epoch 81/150\n",
      "856/856 [==============================] - 0s 118us/sample - loss: 0.4005 - acc: 0.8493 - val_loss: 0.3859 - val_acc: 0.8465\n",
      "Epoch 82/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4144 - acc: 0.8400 - val_loss: 0.3845 - val_acc: 0.8465\n",
      "Epoch 83/150\n",
      "856/856 [==============================] - 0s 130us/sample - loss: 0.4013 - acc: 0.8575 - val_loss: 0.3858 - val_acc: 0.8512\n",
      "Epoch 84/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4023 - acc: 0.8470 - val_loss: 0.3864 - val_acc: 0.8605\n",
      "Epoch 85/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4181 - acc: 0.8493 - val_loss: 0.3887 - val_acc: 0.8558\n",
      "Epoch 86/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.3898 - acc: 0.8563 - val_loss: 0.3865 - val_acc: 0.8465\n",
      "Epoch 87/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.4157 - acc: 0.8505 - val_loss: 0.3884 - val_acc: 0.8605\n",
      "Epoch 88/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.3865 - acc: 0.8575 - val_loss: 0.3918 - val_acc: 0.8419\n",
      "Epoch 89/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3932 - acc: 0.8645 - val_loss: 0.3866 - val_acc: 0.8512\n",
      "Epoch 90/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.3904 - acc: 0.8505 - val_loss: 0.3877 - val_acc: 0.8558\n",
      "Epoch 91/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4008 - acc: 0.8621 - val_loss: 0.3895 - val_acc: 0.8558\n",
      "Epoch 92/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.3880 - acc: 0.8586 - val_loss: 0.3963 - val_acc: 0.8419\n",
      "Epoch 93/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.3945 - acc: 0.8446 - val_loss: 0.3917 - val_acc: 0.8605\n",
      "Epoch 94/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4025 - acc: 0.8586 - val_loss: 0.3948 - val_acc: 0.8512\n",
      "Epoch 95/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.4088 - acc: 0.8493 - val_loss: 0.3918 - val_acc: 0.8558\n",
      "Epoch 96/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.4078 - acc: 0.8435 - val_loss: 0.3895 - val_acc: 0.8605\n",
      "Epoch 97/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4100 - acc: 0.8505 - val_loss: 0.3908 - val_acc: 0.8605\n",
      "Epoch 98/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.3904 - acc: 0.8586 - val_loss: 0.3916 - val_acc: 0.8651\n",
      "Epoch 99/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.3851 - acc: 0.8575 - val_loss: 0.3897 - val_acc: 0.8651\n",
      "Epoch 100/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4027 - acc: 0.8470 - val_loss: 0.3911 - val_acc: 0.8651\n",
      "Epoch 101/150\n",
      "856/856 [==============================] - 0s 120us/sample - loss: 0.3848 - acc: 0.8575 - val_loss: 0.3915 - val_acc: 0.8605\n",
      "Epoch 102/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4042 - acc: 0.8470 - val_loss: 0.3880 - val_acc: 0.8512\n",
      "Epoch 103/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.3996 - acc: 0.8598 - val_loss: 0.3910 - val_acc: 0.8605\n",
      "Epoch 104/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.3901 - acc: 0.8598 - val_loss: 0.3910 - val_acc: 0.8512\n",
      "Epoch 105/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.3898 - acc: 0.8551 - val_loss: 0.3950 - val_acc: 0.8372\n",
      "Epoch 106/150\n",
      "856/856 [==============================] - 0s 120us/sample - loss: 0.3991 - acc: 0.8540 - val_loss: 0.3917 - val_acc: 0.8605\n",
      "Epoch 107/150\n",
      "856/856 [==============================] - 0s 155us/sample - loss: 0.3999 - acc: 0.8505 - val_loss: 0.3981 - val_acc: 0.8465\n",
      "Epoch 108/150\n",
      "856/856 [==============================] - 0s 122us/sample - loss: 0.4125 - acc: 0.8540 - val_loss: 0.3964 - val_acc: 0.8558\n",
      "Epoch 109/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.4010 - acc: 0.8575 - val_loss: 0.3947 - val_acc: 0.8605\n",
      "Epoch 110/150\n",
      "856/856 [==============================] - 0s 137us/sample - loss: 0.4060 - acc: 0.8540 - val_loss: 0.3888 - val_acc: 0.8419\n",
      "Epoch 111/150\n",
      "856/856 [==============================] - 0s 120us/sample - loss: 0.3957 - acc: 0.8516 - val_loss: 0.3893 - val_acc: 0.8558\n",
      "Epoch 112/150\n",
      "856/856 [==============================] - 0s 117us/sample - loss: 0.3827 - acc: 0.8516 - val_loss: 0.3878 - val_acc: 0.8465\n",
      "Epoch 113/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4235 - acc: 0.8470 - val_loss: 0.3905 - val_acc: 0.8558\n",
      "Epoch 114/150\n",
      "856/856 [==============================] - 0s 107us/sample - loss: 0.3966 - acc: 0.8435 - val_loss: 0.3972 - val_acc: 0.8558\n",
      "Epoch 115/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.3960 - acc: 0.8551 - val_loss: 0.3968 - val_acc: 0.8512\n",
      "Epoch 116/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.3895 - acc: 0.8481 - val_loss: 0.3952 - val_acc: 0.8558\n",
      "Epoch 117/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.3933 - acc: 0.8540 - val_loss: 0.3967 - val_acc: 0.8512\n",
      "Epoch 118/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.3922 - acc: 0.8586 - val_loss: 0.3955 - val_acc: 0.8465\n",
      "Epoch 119/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.3957 - acc: 0.8586 - val_loss: 0.3947 - val_acc: 0.8419\n",
      "Epoch 120/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.3942 - acc: 0.8610 - val_loss: 0.3930 - val_acc: 0.8512\n",
      "Epoch 121/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3824 - acc: 0.8575 - val_loss: 0.3938 - val_acc: 0.8465\n",
      "Epoch 122/150\n",
      "856/856 [==============================] - 0s 114us/sample - loss: 0.3839 - acc: 0.8586 - val_loss: 0.3988 - val_acc: 0.8558\n",
      "Epoch 123/150\n",
      "856/856 [==============================] - 0s 125us/sample - loss: 0.3872 - acc: 0.8610 - val_loss: 0.3951 - val_acc: 0.8512\n",
      "Epoch 124/150\n",
      "856/856 [==============================] - 0s 121us/sample - loss: 0.3977 - acc: 0.8446 - val_loss: 0.3992 - val_acc: 0.8698\n",
      "Epoch 125/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3861 - acc: 0.8586 - val_loss: 0.3970 - val_acc: 0.8465\n",
      "Epoch 126/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4014 - acc: 0.8516 - val_loss: 0.3941 - val_acc: 0.8465\n",
      "Epoch 127/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.3883 - acc: 0.8551 - val_loss: 0.3925 - val_acc: 0.8558\n",
      "Epoch 128/150\n",
      "856/856 [==============================] - 0s 113us/sample - loss: 0.3812 - acc: 0.8575 - val_loss: 0.3928 - val_acc: 0.8512\n",
      "Epoch 129/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3884 - acc: 0.8516 - val_loss: 0.3922 - val_acc: 0.8651\n",
      "Epoch 130/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.3928 - acc: 0.8563 - val_loss: 0.3891 - val_acc: 0.8465\n",
      "Epoch 131/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856/856 [==============================] - 0s 116us/sample - loss: 0.3904 - acc: 0.8586 - val_loss: 0.3881 - val_acc: 0.8512\n",
      "Epoch 132/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3851 - acc: 0.8516 - val_loss: 0.3903 - val_acc: 0.8558\n",
      "Epoch 133/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.3846 - acc: 0.8551 - val_loss: 0.3880 - val_acc: 0.8465\n",
      "Epoch 134/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.3969 - acc: 0.8516 - val_loss: 0.3902 - val_acc: 0.8465\n",
      "Epoch 135/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.3906 - acc: 0.8435 - val_loss: 0.3876 - val_acc: 0.8512\n",
      "Epoch 136/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.4013 - acc: 0.8505 - val_loss: 0.3859 - val_acc: 0.8558\n",
      "Epoch 137/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.3811 - acc: 0.8598 - val_loss: 0.3900 - val_acc: 0.8512\n",
      "Epoch 138/150\n",
      "856/856 [==============================] - 0s 111us/sample - loss: 0.3882 - acc: 0.8633 - val_loss: 0.3905 - val_acc: 0.8698\n",
      "Epoch 139/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.3982 - acc: 0.8540 - val_loss: 0.3938 - val_acc: 0.8605\n",
      "Epoch 140/150\n",
      "856/856 [==============================] - 0s 102us/sample - loss: 0.3853 - acc: 0.8598 - val_loss: 0.3924 - val_acc: 0.8744\n",
      "Epoch 141/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.3800 - acc: 0.8633 - val_loss: 0.3889 - val_acc: 0.8465\n",
      "Epoch 142/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.3924 - acc: 0.8481 - val_loss: 0.3898 - val_acc: 0.8419\n",
      "Epoch 143/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.3818 - acc: 0.8657 - val_loss: 0.3886 - val_acc: 0.8465\n",
      "Epoch 144/150\n",
      "856/856 [==============================] - 0s 112us/sample - loss: 0.3924 - acc: 0.8493 - val_loss: 0.3891 - val_acc: 0.8465\n",
      "Epoch 145/150\n",
      "856/856 [==============================] - 0s 105us/sample - loss: 0.3742 - acc: 0.8750 - val_loss: 0.3930 - val_acc: 0.8512\n",
      "Epoch 146/150\n",
      "856/856 [==============================] - 0s 104us/sample - loss: 0.3738 - acc: 0.8540 - val_loss: 0.3917 - val_acc: 0.8558\n",
      "Epoch 147/150\n",
      "856/856 [==============================] - 0s 109us/sample - loss: 0.3947 - acc: 0.8575 - val_loss: 0.3926 - val_acc: 0.8512\n",
      "Epoch 148/150\n",
      "856/856 [==============================] - 0s 110us/sample - loss: 0.4005 - acc: 0.8364 - val_loss: 0.3982 - val_acc: 0.8651\n",
      "Epoch 149/150\n",
      "856/856 [==============================] - 0s 106us/sample - loss: 0.4005 - acc: 0.8470 - val_loss: 0.3899 - val_acc: 0.8558\n",
      "Epoch 150/150\n",
      "856/856 [==============================] - 0s 116us/sample - loss: 0.3830 - acc: 0.8505 - val_loss: 0.3888 - val_acc: 0.8605\n"
     ]
    }
   ],
   "source": [
    "#---------------This section defines our UI----------------------#\n",
    "#----------------Step 1:---------------------------------------#\n",
    "root = tk.Tk()\n",
    "root.title('Neural Networks GUI - Great Learning')\n",
    "tk.Label(root, text='Step 1: File Name').grid(row=5, column=0)\n",
    "v = tk.StringVar()\n",
    "csv_check = tk.StringVar()\n",
    "entry = tk.Entry(root, textvariable=v).grid(row=5, column=1)\n",
    "tk.Button(root, text='Import Data',command=import_csv_data).grid(row=5, column=2)\n",
    "entry2 = tk.Entry(root, textvariable=csv_check).grid(row=5, column=3)\n",
    "#tk.Button(root, text='Close',command=root.destroy).grid(row=10, column=5)\n",
    "#----------------Step 2:-----------------#\n",
    "tk.Label(root, text='Step 2: Target Column').grid(row=7, column=0)\n",
    "target = tk.StringVar()\n",
    "target_found = tk.StringVar()\n",
    "userentry = tk.Entry(root, textvariable=target).grid(row=7, column=1)\n",
    "tk.Button(root, text='Import Target',command=check_target).grid(row=7, column=2)\n",
    "found_or_not = tk.Entry(root, textvariable=target_found).grid(row=7, column=3)\n",
    "#----------------Step 3:-----------------#\n",
    "#---------------------Regressor GUI--------------------------#\n",
    "tk.Label(root, text='Step 3: Neural Network Regressor').grid(row=8, column=0)\n",
    "tk.Label(root, text='Regression').grid(row=9, column=0)\n",
    "tk.Label(root, text='Pickle').grid(row=10, column=0)\n",
    "trained = tk.StringVar()\n",
    "saved = tk.StringVar()\n",
    "tk.Button(root, text='Train',command=regressor_train).grid(row=9, column=1)\n",
    "tr_entry = tk.Entry(root, textvariable=trained).grid(row=9, column=2)\n",
    "tk.Button(root, text='Run',command=save_regressor_model).grid(row=10, column=1)\n",
    "savedentry = tk.Entry(root, textvariable=saved).grid(row=10, column=2)\n",
    "#-------------Step 4------------------------------#\n",
    "#-----------------Classifier GUI-----------------------#\n",
    "tk.Label(root, text='Step 3: Neural Network Classifier').grid(row=11, column=0)\n",
    "tk.Label(root, text='Classification').grid(row=12, column=0)\n",
    "tk.Label(root, text='Pickle').grid(row=13, column=0)\n",
    "c_trained = tk.StringVar()\n",
    "c_saved = tk.StringVar()\n",
    "tk.Button(root, text='Train',command=classifier_train).grid(row=12, column=1)\n",
    "tr_entry_c = tk.Entry(root, textvariable=c_trained).grid(row=12, column=2)\n",
    "tk.Button(root, text='Run',command=save_classifier_model).grid(row=13, column=1)\n",
    "saved_c = tk.Entry(root, textvariable=c_saved).grid(row=13, column=2)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
